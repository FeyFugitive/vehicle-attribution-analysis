#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´è½¦è®¢å•å½’å› åˆ†æ - ç»¼åˆç‰ˆæœ¬
åŒ…å«æ•°æ®æ¢ç´¢ã€æ¸ é“åˆ†æã€çœä»½åˆ†æã€è½¦ç³»åˆ†æç­‰å¤šä¸ªç»´åº¦
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class VehicleAttributionAnalysis:
    def __init__(self, data_file):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.data_file = data_file
        self.df = None
        self.analysis_results = {}
        
    def load_and_clean_data(self):
        """åŠ è½½å’Œæ¸…æ´—æ•°æ®"""
        print("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®...")
        self.df = pd.read_excel(self.data_file, engine="openpyxl")
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.df.shape}")
        
        # æ•°æ®æ¸…æ´— - æ–¹æ¡ˆBï¼šç§»é™¤è·³è·ƒå¼è®¢å•
        jump = (
            (self.df["deposit_payment_time"].notna() & self.df["intention_payment_time"].isna()) |
            (self.df["lock_time"].notna() & self.df["deposit_payment_time"].isna()) |
            (self.df["final_payment_time"].notna() & self.df["lock_time"].isna()) |
            (self.df["delivery_date"].notna() & self.df["final_payment_time"].isna())
        )
        self.df = self.df[~jump]
        print(f"æ¸…æ´—åæ•°æ®å½¢çŠ¶: {self.df.shape}")
        
        # æ·»åŠ æ¸ é“åˆ†ç±»
        def categorize_channel(channel):
            if channel == "é—¨åº—":
                return "STORE"
            elif channel == "æ€»éƒ¨":
                return "HQ"
            else:
                return "OTHER"
        
        self.df["channel_category"] = self.df["big_channel_name"].map(categorize_channel).fillna("OTHER")
        
        return self.df
    
    def basic_data_exploration(self):
        """åŸºç¡€æ•°æ®æ¢ç´¢"""
        print("\nğŸ” åŸºç¡€æ•°æ®æ¢ç´¢")
        print("=" * 50)
        
        # è®¢å•çŠ¶æ€åˆ†å¸ƒ
        print("è®¢å•çŠ¶æ€åˆ†å¸ƒ:")
        status_counts = self.df["order_status"].value_counts()
        print(status_counts)
        
        # è½¦ç³»åˆ†å¸ƒ
        print("\nè½¦ç³»åˆ†å¸ƒ (Top 10):")
        series_counts = self.df["series"].value_counts().head(10)
        print(series_counts)
        
        # çœä»½åˆ†å¸ƒ
        print("\nçœä»½åˆ†å¸ƒ (Top 10):")
        province_counts = self.df["province_name"].value_counts().head(10)
        print(province_counts)
        
        # æ¸ é“åˆ†å¸ƒ
        print("\næ¸ é“åˆ†å¸ƒ:")
        channel_counts = self.df["channel_category"].value_counts()
        print(channel_counts)
        
        # è½¬åŒ–æ¼æ–—
        print("\nè½¬åŒ–æ¼æ–—:")
        funnel_data = {
            "å¿ƒæ„¿å•": self.df["wish_create_time"].notna().sum(),
            "æ„å‘é‡‘": self.df["intention_payment_time"].notna().sum(),
            "å®šé‡‘": self.df["deposit_payment_time"].notna().sum(),
            "é”å•": self.df["lock_time"].notna().sum(),
            "å°¾æ¬¾": self.df["final_payment_time"].notna().sum(),
            "äº¤ä»˜": self.df["delivery_date"].notna().sum()
        }
        
        for stage, count in funnel_data.items():
            rate = count / funnel_data["å¿ƒæ„¿å•"] * 100
            print(f"{stage}: {count:,} ({rate:.1f}%)")
        
        self.analysis_results["funnel_data"] = funnel_data
        return funnel_data
    
    def markov_attribution_analysis(self):
        """é©¬å°”å¯å¤«é“¾å½’å› åˆ†æ"""
        print("\nğŸ“ˆ é©¬å°”å¯å¤«é“¾å½’å› åˆ†æ")
        print("=" * 50)
        
        # å®šä¹‰é˜¶æ®µ
        STAGE_COLS = [
            ("wish_create_time", "Wish"),
            ("intention_payment_time", "Intention"),
            ("deposit_payment_time", "Deposit"),
            ("lock_time", "Lock"),
            ("final_payment_time", "Final"),
            ("delivery_date", "Delivery"),
        ]
        
        # æ„å»ºè·¯å¾„
        def build_paths(df, category_col):
            def row_path(row):
                path = ["Start"]
                category = row[category_col]
                for col, stage in STAGE_COLS:
                    if pd.notna(row[col]):
                        path.append(f"{stage}_{category}")
                path.append("Conversion" if pd.notna(row["delivery_date"]) else "Null")
                return path
            
            return df.apply(row_path, axis=1).tolist()
        
        # è®¡ç®—ç§»é™¤æ•ˆåº”
        def removal_effect(paths, test_nodes):
            states = sorted({s for p in paths for s in p})
            idx = {s: i for i, s in enumerate(states)}
            n = len(states)
            
            # è½¬ç§»çŸ©é˜µ
            T = np.zeros((n, n))
            for p in paths:
                for a, b in zip(p[:-1], p[1:]):
                    T[idx[a], idx[b]] += 1
            row_sum = T.sum(1, keepdims=True)
            T = np.divide(T, row_sum, out=np.zeros_like(T), where=row_sum != 0)
            
            # åŸºå‡†è½¬åŒ–æ¦‚ç‡
            absorb = ["Conversion", "Null"]
            trans = [s for s in states if s not in absorb]
            Q = T[np.ix_([idx[s] for s in trans], [idx[s] for s in trans])]
            R = T[np.ix_([idx[s] for s in trans], [idx[s] for s in absorb])]
            N = np.linalg.inv(np.eye(len(Q)) - Q)
            v = np.zeros(len(trans))
            v[trans.index("Start")] = 1
            baseline = (v @ N @ R)[0]
            
            results = []
            for node in test_nodes:
                if node not in idx:
                    continue
                Ti = T.copy()
                Ti[:, idx[node]] = 0
                Ti[idx[node], :] = 0
                rs = Ti.sum(1, keepdims=True)
                Ti = np.divide(Ti, rs, out=np.zeros_like(Ti), where=rs != 0)
                
                Q2 = Ti[np.ix_([idx[s] for s in trans], [idx[s] for s in trans])]
                R2 = Ti[np.ix_([idx[s] for s in trans], [idx[s] for s in absorb])]
                N2 = np.linalg.inv(np.eye(len(Q2)) - Q2)
                new_conv = (v @ N2 @ R2)[0]
                
                results.append((node, round((baseline - new_conv) * 100, 2)))
            return results
        
        # æ¸ é“åˆ†æ
        print("æ¸ é“å½’å› åˆ†æ:")
        paths_channel = build_paths(self.df, "channel_category")
        channel_nodes = [f"{stage}_{ch}" for ch in ["HQ", "STORE"] for _, stage in STAGE_COLS[1:]]
        channel_effects = removal_effect(paths_channel, channel_nodes)
        
        for node, effect in sorted(channel_effects, key=lambda x: -x[1]):
            print(f"  {node:<20}: {effect:+.2f} pp")
        
        # çœä»½åˆ†æ
        print("\nçœä»½å½’å› åˆ†æ (Top 8):")
        top_provinces = self.df["province_name"].value_counts().head(8).index.tolist()
        self.df["province_cat"] = self.df["province_name"].fillna("UNKNOWN").apply(
            lambda x: x if x in top_provinces else "OTHER"
        )
        
        paths_province = build_paths(self.df, "province_cat")
        province_nodes = [f"{stage}_{prov}" for prov in top_provinces for _, stage in STAGE_COLS[1:]]
        province_effects = removal_effect(paths_province, province_nodes)
        
        for node, effect in sorted(province_effects, key=lambda x: -x[1])[:10]:
            print(f"  {node:<20}: {effect:+.2f} pp")
        
        # è½¦ç³»åˆ†æ
        print("\nè½¦ç³»å½’å› åˆ†æ (Top 8):")
        top_series = self.df["series"].value_counts().head(8).index.tolist()
        self.df["series_cat"] = self.df["series"].fillna("UNKNOWN").apply(
            lambda x: x if x in top_series else "OTHER"
        )
        
        paths_series = build_paths(self.df, "series_cat")
        series_nodes = [f"{stage}_{series}" for series in top_series for _, stage in STAGE_COLS[1:]]
        series_effects = removal_effect(paths_series, series_nodes)
        
        for node, effect in sorted(series_effects, key=lambda x: -x[1])[:10]:
            print(f"  {node:<20}: {effect:+.2f} pp")
        
        self.analysis_results["channel_effects"] = channel_effects
        self.analysis_results["province_effects"] = province_effects
        self.analysis_results["series_effects"] = series_effects
        
        return channel_effects, province_effects, series_effects
    
    def time_series_analysis(self):
        """æ—¶é—´åºåˆ—åˆ†æ"""
        print("\nâ° æ—¶é—´åºåˆ—åˆ†æ")
        print("=" * 50)
        
        # æŒ‰æœˆç»Ÿè®¡è®¢å•é‡
        self.df["order_month"] = pd.to_datetime(self.df["order_create_time"]).dt.to_period('M')
        monthly_orders = self.df.groupby("order_month").size()
        
        print("æœˆåº¦è®¢å•é‡è¶‹åŠ¿:")
        for month, count in monthly_orders.tail(6).items():
            print(f"  {month}: {count:,} è®¢å•")
        
        # è½¬åŒ–ç‡æ—¶é—´è¶‹åŠ¿
        monthly_conversion = self.df.groupby("order_month").apply(
            lambda x: x["delivery_date"].notna().sum() / len(x) * 100
        )
        
        print("\næœˆåº¦è½¬åŒ–ç‡è¶‹åŠ¿:")
        for month, rate in monthly_conversion.tail(6).items():
            print(f"  {month}: {rate:.1f}%")
        
        self.analysis_results["monthly_orders"] = monthly_orders
        self.analysis_results["monthly_conversion"] = monthly_conversion
        
        return monthly_orders, monthly_conversion
    
    def cancellation_analysis(self):
        """é€€è®¢åˆ†æ"""
        print("\nâŒ é€€è®¢åˆ†æ")
        print("=" * 50)
        
        # é€€è®¢ç‡ç»Ÿè®¡
        total_orders = len(self.df)
        intention_refunds = self.df["intention_refund_time"].notna().sum()
        deposit_refunds = self.df["deposit_refund_time"].notna().sum()
        
        print(f"æ€»è®¢å•æ•°: {total_orders:,}")
        print(f"æ„å‘é‡‘é€€è®¢æ•°: {intention_refunds:,} ({intention_refunds/total_orders*100:.1f}%)")
        print(f"å®šé‡‘é€€è®¢æ•°: {deposit_refunds:,} ({deposit_refunds/total_orders*100:.1f}%)")
        
        # é€€è®¢åŸå› åˆ†æ
        if "hold_reason" in self.df.columns:
            hold_reasons = self.df["hold_reason"].value_counts()
            print("\nHoldåŸå› åˆ†å¸ƒ:")
            for reason, count in hold_reasons.head(5).items():
                print(f"  {reason}: {count}")
        
        self.analysis_results["refund_stats"] = {
            "total_orders": total_orders,
            "intention_refunds": intention_refunds,
            "deposit_refunds": deposit_refunds
        }
        
        return self.analysis_results["refund_stats"]
    
    def generate_summary_report(self):
        """ç”Ÿæˆåˆ†ææ€»ç»“"""
        print("\nğŸ“‹ åˆ†ææ€»ç»“")
        print("=" * 50)
        
        summary = {
            "æ•°æ®æ¦‚è§ˆ": {
                "æ€»è®¢å•æ•°": f"{len(self.df):,}",
                "è½¬åŒ–ç‡": f"{self.df['delivery_date'].notna().sum()/len(self.df)*100:.1f}%",
                "ä¸»è¦æ¸ é“": self.df["channel_category"].value_counts().index[0],
                "ä¸»è¦è½¦ç³»": self.df["series"].value_counts().index[0],
                "ä¸»è¦çœä»½": self.df["province_name"].value_counts().index[0]
            },
            "å…³é”®å‘ç°": []
        }
        
        # æ·»åŠ å…³é”®å‘ç°
        if "channel_effects" in self.analysis_results:
            top_channel = max(self.analysis_results["channel_effects"], key=lambda x: x[1])
            summary["å…³é”®å‘ç°"].append(f"æ¸ é“è´¡çŒ®æœ€å¤§çš„æ˜¯ {top_channel[0]}ï¼Œç§»é™¤æ•ˆåº”ä¸º {top_channel[1]:+.2f} pp")
        
        if "province_effects" in self.analysis_results:
            top_province = max(self.analysis_results["province_effects"], key=lambda x: x[1])
            summary["å…³é”®å‘ç°"].append(f"çœä»½è´¡çŒ®æœ€å¤§çš„æ˜¯ {top_province[0]}ï¼Œç§»é™¤æ•ˆåº”ä¸º {top_province[1]:+.2f} pp")
        
        if "series_effects" in self.analysis_results:
            top_series = max(self.analysis_results["series_effects"], key=lambda x: x[1])
            summary["å…³é”®å‘ç°"].append(f"è½¦ç³»è´¡çŒ®æœ€å¤§çš„æ˜¯ {top_series[0]}ï¼Œç§»é™¤æ•ˆåº”ä¸º {top_series[1]:+.2f} pp")
        
        print("æ•°æ®æ¦‚è§ˆ:")
        for key, value in summary["æ•°æ®æ¦‚è§ˆ"].items():
            print(f"  {key}: {value}")
        
        print("\nå…³é”®å‘ç°:")
        for finding in summary["å…³é”®å‘ç°"]:
            print(f"  â€¢ {finding}")
        
        return summary
    
    def save_results(self):
        """ä¿å­˜åˆ†æç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        
        # ä¿å­˜æ¸ é“åˆ†æç»“æœ
        if "channel_effects" in self.analysis_results:
            channel_df = pd.DataFrame([
                {"Channel": node.split("_")[-1], "Stage": "_".join(node.split("_")[:-1]), 
                 "Removal_Effect_pp": effect} 
                for node, effect in self.analysis_results["channel_effects"]
            ])
            channel_df.to_csv("data/channel_removal_effects.csv", index=False)
            print("âœ“ æ¸ é“åˆ†æç»“æœå·²ä¿å­˜åˆ° data/channel_removal_effects.csv")
        
        # ä¿å­˜çœä»½åˆ†æç»“æœ
        if "province_effects" in self.analysis_results:
            province_df = pd.DataFrame([
                {"Province": node.split("_")[-1], "Stage": "_".join(node.split("_")[:-1]), 
                 "Removal_Effect_pp": effect} 
                for node, effect in self.analysis_results["province_effects"]
            ])
            province_df.to_csv("data/province_removal_effects.csv", index=False)
            print("âœ“ çœä»½åˆ†æç»“æœå·²ä¿å­˜åˆ° data/province_removal_effects.csv")
        
        # ä¿å­˜è½¦ç³»åˆ†æç»“æœ
        if "series_effects" in self.analysis_results:
            series_df = pd.DataFrame([
                {"Series": node.split("_")[-1], "Stage": "_".join(node.split("_")[:-1]), 
                 "Removal_Effect_pp": effect} 
                for node, effect in self.analysis_results["series_effects"]
            ])
            series_df.to_csv("data/series_removal_effects.csv", index=False)
            print("âœ“ è½¦ç³»åˆ†æç»“æœå·²ä¿å­˜åˆ° data/series_removal_effects.csv")
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹æ•´è½¦è®¢å•å½’å› åˆ†æ")
        print("=" * 60)
        
        # 1. åŠ è½½å’Œæ¸…æ´—æ•°æ®
        self.load_and_clean_data()
        
        # 2. åŸºç¡€æ•°æ®æ¢ç´¢
        self.basic_data_exploration()
        
        # 3. é©¬å°”å¯å¤«é“¾å½’å› åˆ†æ
        self.markov_attribution_analysis()
        
        # 4. æ—¶é—´åºåˆ—åˆ†æ
        self.time_series_analysis()
        
        # 5. é€€è®¢åˆ†æ
        self.cancellation_analysis()
        
        # 6. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary = self.generate_summary_report()
        
        # 7. ä¿å­˜ç»“æœ
        self.save_results()
        
        print("\nâœ… åˆ†æå®Œæˆï¼")
        return summary

if __name__ == "__main__":
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    data_file = "data/æ•´è½¦è®¢å•çŠ¶æ€æŒ‡æ ‡è¡¨.xlsx"
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = VehicleAttributionAnalysis(data_file)
    results = analyzer.run_full_analysis() 