#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´è½¦è®¢å•å½’å› åˆ†æ - ä¼˜åŒ–ç‰ˆæœ¬
åŒ…å«æ‰€æœ‰æ”¹è¿›ï¼šè·¯å¾„æ„å»ºä¼˜åŒ–ã€ç©ºå€¼å¤„ç†å¢å¼ºã€æ€§èƒ½ä¼˜åŒ–ã€å¼‚å¸¸å¤„ç†å¼ºåŒ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import ANALYSIS_CONFIG, VISUALIZATION_CONFIG, DATA_CONFIG, OUTPUT_CONFIG
from utils import (
    build_optimized_paths, 
    removal_effect_optimized, 
    safe_category_mapping,
    validate_data_quality,
    parallel_removal_effect,
    log_analysis_progress
)

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = VISUALIZATION_CONFIG['CHINESE_FONT']
plt.rcParams['axes.unicode_minus'] = False

class OptimizedVehicleAttributionAnalysis:
    """ä¼˜åŒ–ç‰ˆæœ¬çš„æ•´è½¦è®¢å•å½’å› åˆ†æå™¨"""
    
    def __init__(self, data_file):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.data_file = data_file
        self.df = None
        self.analysis_results = {}
        self.validation_results = {}
        
    def load_and_clean_data(self):
        """åŠ è½½å’Œæ¸…æ´—æ•°æ®"""
        log_analysis_progress("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®...")
        
        try:
            self.df = pd.read_excel(self.data_file, engine="openpyxl")
            log_analysis_progress(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.df.shape}")
            
            # æ•°æ®è´¨é‡éªŒè¯
            self.validation_results = validate_data_quality(self.df)
            log_analysis_progress(f"æ•°æ®è´¨é‡éªŒè¯å®Œæˆ: {self.validation_results['total_rows']} è¡Œæ•°æ®")
            
            if self.validation_results['missing_values']:
                log_analysis_progress(f"å‘ç°ç¼ºå¤±å€¼: {self.validation_results['missing_values']}")
            
            # æ•°æ®æ¸…æ´— - æ–¹æ¡ˆBï¼šç§»é™¤è·³è·ƒå¼è®¢å•
            if DATA_CONFIG['CLEAN_JUMP_ORDERS']:
                jump = (
                    (self.df["deposit_payment_time"].notna() & self.df["intention_payment_time"].isna()) |
                    (self.df["lock_time"].notna() & self.df["deposit_payment_time"].isna()) |
                    (self.df["final_payment_time"].notna() & self.df["lock_time"].isna()) |
                    (self.df["delivery_date"].notna() & self.df["final_payment_time"].isna())
                )
                self.df = self.df[~jump]
                log_analysis_progress(f"æ¸…æ´—åæ•°æ®å½¢çŠ¶: {self.df.shape}")
            
            # æ·»åŠ æ¸ é“åˆ†ç±»
            if DATA_CONFIG['NORMALIZE_CATEGORIES']:
                self.df["channel_category"] = self.df["big_channel_name"].apply(
                    lambda x: safe_category_mapping(x, ANALYSIS_CONFIG['CHANNEL_MAPPING'])
                )
            
            return self.df
            
        except Exception as e:
            log_analysis_progress(f"æ•°æ®åŠ è½½å¤±è´¥: {e}", 'error')
            raise
    
    def basic_data_exploration(self):
        """åŸºç¡€æ•°æ®æ¢ç´¢"""
        log_analysis_progress("\nğŸ” åŸºç¡€æ•°æ®æ¢ç´¢")
        print("=" * 50)
        
        # è®¢å•çŠ¶æ€åˆ†å¸ƒ
        print("è®¢å•çŠ¶æ€åˆ†å¸ƒ:")
        status_counts = self.df["order_status"].value_counts()
        print(status_counts)
        
        # è½¦ç³»åˆ†å¸ƒ
        print("\nè½¦ç³»åˆ†å¸ƒ (Top 10):")
        series_counts = self.df["series"].value_counts().head(10)
        print(series_counts)
        
        # çœä»½åˆ†å¸ƒ
        print("\nçœä»½åˆ†å¸ƒ (Top 10):")
        province_counts = self.df["province_name"].value_counts().head(10)
        print(province_counts)
        
        # æ¸ é“åˆ†å¸ƒ
        print("\næ¸ é“åˆ†å¸ƒ:")
        channel_counts = self.df["channel_category"].value_counts()
        print(channel_counts)
        
        # è½¬åŒ–æ¼æ–—
        print("\nè½¬åŒ–æ¼æ–—:")
        funnel_data = {
            "å¿ƒæ„¿å•": self.df["wish_create_time"].notna().sum(),
            "æ„å‘é‡‘": self.df["intention_payment_time"].notna().sum(),
            "å®šé‡‘": self.df["deposit_payment_time"].notna().sum(),
            "é”å•": self.df["lock_time"].notna().sum(),
            "å°¾æ¬¾": self.df["final_payment_time"].notna().sum(),
            "äº¤ä»˜": self.df["delivery_date"].notna().sum()
        }
        
        for stage, count in funnel_data.items():
            rate = count / funnel_data["å¿ƒæ„¿å•"] * 100
            print(f"{stage}: {count:,} ({rate:.1f}%)")
        
        self.analysis_results["funnel_data"] = funnel_data
        return funnel_data
    
    def markov_attribution_analysis(self):
        """é©¬å°”å¯å¤«é“¾å½’å› åˆ†æ"""
        log_analysis_progress("\nğŸ¯ é©¬å°”å¯å¤«é“¾å½’å› åˆ†æ")
        print("=" * 50)
        
        # æ¸ é“åˆ†æ
        log_analysis_progress("æ¸ é“å½’å› åˆ†æ...")
        paths_channel = build_optimized_paths(self.df, "channel_category")
        channel_nodes = [
            f"{stage}{ANALYSIS_CONFIG['PATH_SEPARATOR']}{ch}" 
            for ch in ["HQ", "STORE"] 
            for _, stage in ANALYSIS_CONFIG['STAGE_COLS'][1:]
        ]
        
        channel_effects = removal_effect_optimized(
            paths_channel, 
            channel_nodes, 
            use_sparse=DATA_CONFIG['USE_SPARSE_MATRIX']
        )
        
        print("æ¸ é“å½’å› åˆ†æ:")
        for node, effect in sorted(channel_effects, key=lambda x: -x[1]):
            print(f"  {node:<25}: {effect:+.2f} pp")
        
        # çœä»½åˆ†æ
        log_analysis_progress("çœä»½å½’å› åˆ†æ...")
        top_provinces = self.df["province_name"].value_counts().head(ANALYSIS_CONFIG['TOP_PROVINCES']).index.tolist()
        self.df["province_cat"] = self.df["province_name"].apply(
            lambda x: safe_category_mapping(x, {p: p for p in top_provinces})
        )
        
        paths_province = build_optimized_paths(self.df, "province_cat")
        province_nodes = [
            f"{stage}{ANALYSIS_CONFIG['PATH_SEPARATOR']}{prov}" 
            for prov in top_provinces 
            for _, stage in ANALYSIS_CONFIG['STAGE_COLS'][1:]
        ]
        
        province_effects = removal_effect_optimized(
            paths_province, 
            province_nodes, 
            use_sparse=DATA_CONFIG['USE_SPARSE_MATRIX']
        )
        
        print(f"\nçœä»½å½’å› åˆ†æ (Top {ANALYSIS_CONFIG['TOP_PROVINCES']}):")
        for node, effect in sorted(province_effects, key=lambda x: -x[1])[:10]:
            print(f"  {node:<25}: {effect:+.2f} pp")
        
        # è½¦ç³»åˆ†æ
        log_analysis_progress("è½¦ç³»å½’å› åˆ†æ...")
        top_series = self.df["series"].value_counts().head(ANALYSIS_CONFIG['TOP_SERIES']).index.tolist()
        self.df["series_cat"] = self.df["series"].apply(
            lambda x: safe_category_mapping(x, {s: s for s in top_series})
        )
        
        paths_series = build_optimized_paths(self.df, "series_cat")
        series_nodes = [
            f"{stage}{ANALYSIS_CONFIG['PATH_SEPARATOR']}{series}" 
            for series in top_series 
            for _, stage in ANALYSIS_CONFIG['STAGE_COLS'][1:]
        ]
        
        series_effects = removal_effect_optimized(
            paths_series, 
            series_nodes, 
            use_sparse=DATA_CONFIG['USE_SPARSE_MATRIX']
        )
        
        print(f"\nè½¦ç³»å½’å› åˆ†æ (Top {ANALYSIS_CONFIG['TOP_SERIES']}):")
        for node, effect in sorted(series_effects, key=lambda x: -x[1])[:10]:
            print(f"  {node:<25}: {effect:+.2f} pp")
        
        self.analysis_results["channel_effects"] = channel_effects
        self.analysis_results["province_effects"] = province_effects
        self.analysis_results["series_effects"] = series_effects
        
        return channel_effects, province_effects, series_effects
    
    def parallel_analysis(self):
        """å¹¶è¡Œåˆ†æï¼ˆå¤§æ•°æ®é›†ä¼˜åŒ–ï¼‰"""
        log_analysis_progress("\nâš¡ å¹¶è¡Œå½’å› åˆ†æ")
        print("=" * 50)
        
        # å‡†å¤‡å¹¶è¡Œåˆ†ææ•°æ®
        paths_list = []
        nodes_list = []
        
        # æ¸ é“åˆ†æ
        paths_channel = build_optimized_paths(self.df, "channel_category")
        channel_nodes = [
            f"{stage}{ANALYSIS_CONFIG['PATH_SEPARATOR']}{ch}" 
            for ch in ["HQ", "STORE"] 
            for _, stage in ANALYSIS_CONFIG['STAGE_COLS'][1:]
        ]
        paths_list.append(paths_channel)
        nodes_list.append(channel_nodes)
        
        # çœä»½åˆ†æ
        top_provinces = self.df["province_name"].value_counts().head(ANALYSIS_CONFIG['TOP_PROVINCES']).index.tolist()
        self.df["province_cat"] = self.df["province_name"].apply(
            lambda x: safe_category_mapping(x, {p: p for p in top_provinces})
        )
        paths_province = build_optimized_paths(self.df, "province_cat")
        province_nodes = [
            f"{stage}{ANALYSIS_CONFIG['PATH_SEPARATOR']}{prov}" 
            for prov in top_provinces 
            for _, stage in ANALYSIS_CONFIG['STAGE_COLS'][1:]
        ]
        paths_list.append(paths_province)
        nodes_list.append(province_nodes)
        
        # è½¦ç³»åˆ†æ
        top_series = self.df["series"].value_counts().head(ANALYSIS_CONFIG['TOP_SERIES']).index.tolist()
        self.df["series_cat"] = self.df["series"].apply(
            lambda x: safe_category_mapping(x, {s: s for s in top_series})
        )
        paths_series = build_optimized_paths(self.df, "series_cat")
        series_nodes = [
            f"{stage}{ANALYSIS_CONFIG['PATH_SEPARATOR']}{series}" 
            for series in top_series 
            for _, stage in ANALYSIS_CONFIG['STAGE_COLS'][1:]
        ]
        paths_list.append(paths_series)
        nodes_list.append(series_nodes)
        
        # æ‰§è¡Œå¹¶è¡Œåˆ†æ
        results = parallel_removal_effect(paths_list, nodes_list)
        
        if len(results) >= 3:
            channel_effects, province_effects, series_effects = results[:3]
            
            print("å¹¶è¡Œåˆ†æç»“æœ:")
            print("æ¸ é“å½’å› :")
            for node, effect in sorted(channel_effects, key=lambda x: -x[1]):
                print(f"  {node:<25}: {effect:+.2f} pp")
            
            print(f"\nçœä»½å½’å›  (Top {ANALYSIS_CONFIG['TOP_PROVINCES']}):")
            for node, effect in sorted(province_effects, key=lambda x: -x[1])[:10]:
                print(f"  {node:<25}: {effect:+.2f} pp")
            
            print(f"\nè½¦ç³»å½’å›  (Top {ANALYSIS_CONFIG['TOP_SERIES']}):")
            for node, effect in sorted(series_effects, key=lambda x: -x[1])[:10]:
                print(f"  {node:<25}: {effect:+.2f} pp")
            
            self.analysis_results["parallel_channel_effects"] = channel_effects
            self.analysis_results["parallel_province_effects"] = province_effects
            self.analysis_results["parallel_series_effects"] = series_effects
            
            return channel_effects, province_effects, series_effects
        
        return [], [], []
    
    def time_series_analysis(self):
        """æ—¶é—´åºåˆ—åˆ†æ"""
        log_analysis_progress("\nâ° æ—¶é—´åºåˆ—åˆ†æ")
        print("=" * 50)
        
        # æŒ‰æœˆç»Ÿè®¡è®¢å•é‡
        self.df["order_month"] = pd.to_datetime(self.df["order_create_time"]).dt.to_period('M')
        monthly_orders = self.df.groupby("order_month").size()
        
        print("æœˆåº¦è®¢å•é‡è¶‹åŠ¿:")
        for month, count in monthly_orders.tail(6).items():
            print(f"  {month}: {count:,} è®¢å•")
        
        # æœˆåº¦è½¬åŒ–ç‡
        monthly_conversion = self.df.groupby("order_month").apply(
            lambda x: x["delivery_date"].notna().sum() / len(x) * 100
        )
        
        print("\næœˆåº¦è½¬åŒ–ç‡è¶‹åŠ¿:")
        for month, rate in monthly_conversion.tail(6).items():
            print(f"  {month}: {rate:.2f}%")
        
        self.analysis_results["monthly_orders"] = monthly_orders
        self.analysis_results["monthly_conversion"] = monthly_conversion
        
        return monthly_orders, monthly_conversion
    
    def cancellation_analysis(self):
        """é€€è®¢åˆ†æ"""
        log_analysis_progress("\nâŒ é€€è®¢åˆ†æ")
        print("=" * 50)
        
        # æ„å‘é‡‘é€€è®¢ç‡
        intention_cancelled = self.df[
            (self.df["intention_payment_time"].notna()) & 
            (self.df["deposit_payment_time"].isna())
        ].shape[0]
        intention_total = self.df["intention_payment_time"].notna().sum()
        intention_cancel_rate = intention_cancelled / intention_total * 100 if intention_total > 0 else 0
        
        print(f"æ„å‘é‡‘é€€è®¢ç‡: {intention_cancel_rate:.2f}% ({intention_cancelled:,}/{intention_total:,})")
        
        # å®šé‡‘é€€è®¢ç‡
        deposit_cancelled = self.df[
            (self.df["deposit_payment_time"].notna()) & 
            (self.df["lock_time"].isna())
        ].shape[0]
        deposit_total = self.df["deposit_payment_time"].notna().sum()
        deposit_cancel_rate = deposit_cancelled / deposit_total * 100 if deposit_total > 0 else 0
        
        print(f"å®šé‡‘é€€è®¢ç‡: {deposit_cancel_rate:.2f}% ({deposit_cancelled:,}/{deposit_total:,})")
        
        # HoldåŸå› åˆ†æ
        if "hold_reason" in self.df.columns:
            hold_reasons = self.df["hold_reason"].value_counts().head(5)
            print("\nHoldåŸå› åˆ†æ (Top 5):")
            for reason, count in hold_reasons.items():
                print(f"  {reason}: {count:,}")
        
        self.analysis_results["intention_cancel_rate"] = intention_cancel_rate
        self.analysis_results["deposit_cancel_rate"] = deposit_cancel_rate
        
        return intention_cancel_rate, deposit_cancel_rate
    
    def generate_summary_report(self):
        """ç”Ÿæˆåˆ†ææ€»ç»“æŠ¥å‘Š"""
        log_analysis_progress("\nğŸ“‹ ç”Ÿæˆåˆ†ææ€»ç»“æŠ¥å‘Š")
        print("=" * 50)
        
        # å…³é”®å‘ç°
        print("ğŸ” å…³é”®å‘ç°:")
        
        if "channel_effects" in self.analysis_results:
            top_channel = max(self.analysis_results["channel_effects"], key=lambda x: x[1])
            print(f"  æ¸ é“è´¡çŒ®æœ€å¤§: {top_channel[0]}, ç§»é™¤æ•ˆåº”ä¸º {top_channel[1]:+.2f} pp")
        
        if "province_effects" in self.analysis_results:
            top_province = max(self.analysis_results["province_effects"], key=lambda x: x[1])
            print(f"  çœä»½è´¡çŒ®æœ€å¤§: {top_province[0]}, ç§»é™¤æ•ˆåº”ä¸º {top_province[1]:+.2f} pp")
        
        if "series_effects" in self.analysis_results:
            top_series = max(self.analysis_results["series_effects"], key=lambda x: x[1])
            print(f"  è½¦ç³»è´¡çŒ®æœ€å¤§: {top_series[0]}, ç§»é™¤æ•ˆåº”ä¸º {top_series[1]:+.2f} pp")
        
        # æ•°æ®è´¨é‡æ€»ç»“
        print(f"\nğŸ“Š æ•°æ®è´¨é‡æ€»ç»“:")
        print(f"  æ€»æ•°æ®è¡Œæ•°: {self.validation_results['total_rows']:,}")
        print(f"  é‡å¤è¡Œæ•°: {self.validation_results['duplicate_rows']:,}")
        if self.validation_results['missing_values']:
            print(f"  ç¼ºå¤±å€¼åˆ—æ•°: {len(self.validation_results['missing_values'])}")
        
        # è½¬åŒ–æ¼æ–—æ€»ç»“
        if "funnel_data" in self.analysis_results:
            funnel = self.analysis_results["funnel_data"]
            print(f"\nğŸ”„ è½¬åŒ–æ¼æ–—æ€»ç»“:")
            print(f"  å¿ƒæ„¿å•åˆ°äº¤ä»˜è½¬åŒ–ç‡: {funnel['äº¤ä»˜']/funnel['å¿ƒæ„¿å•']*100:.1f}%")
            print(f"  æ„å‘é‡‘åˆ°äº¤ä»˜è½¬åŒ–ç‡: {funnel['äº¤ä»˜']/funnel['æ„å‘é‡‘']*100:.1f}%")
    
    def save_results(self):
        """ä¿å­˜åˆ†æç»“æœ"""
        log_analysis_progress("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("data", exist_ok=True)
        os.makedirs("reports", exist_ok=True)
        
        # ä¿å­˜æ¸ é“å½’å› ç»“æœ
        if "channel_effects" in self.analysis_results:
            channel_df = pd.DataFrame(
                self.analysis_results["channel_effects"], 
                columns=["node", "removal_effect"]
            )
            channel_df.to_csv("data/channel_removal_effects.csv", index=False, encoding=OUTPUT_CONFIG['CSV_ENCODING'])
            log_analysis_progress("æ¸ é“å½’å› ç»“æœå·²ä¿å­˜åˆ° data/channel_removal_effects.csv")
        
        # ä¿å­˜çœä»½å½’å› ç»“æœ
        if "province_effects" in self.analysis_results:
            province_df = pd.DataFrame(
                self.analysis_results["province_effects"], 
                columns=["node", "removal_effect"]
            )
            province_df.to_csv("data/province_removal_effects.csv", index=False, encoding=OUTPUT_CONFIG['CSV_ENCODING'])
            log_analysis_progress("çœä»½å½’å› ç»“æœå·²ä¿å­˜åˆ° data/province_removal_effects.csv")
        
        # ä¿å­˜è½¦ç³»å½’å› ç»“æœ
        if "series_effects" in self.analysis_results:
            series_df = pd.DataFrame(
                self.analysis_results["series_effects"], 
                columns=["node", "removal_effect"]
            )
            series_df.to_csv("data/series_removal_effects.csv", index=False, encoding=OUTPUT_CONFIG['CSV_ENCODING'])
            log_analysis_progress("è½¦ç³»å½’å› ç»“æœå·²ä¿å­˜åˆ° data/series_removal_effects.csv")
        
        # ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š
        validation_df = pd.DataFrame([
            {"metric": k, "value": str(v)} 
            for k, v in self.validation_results.items()
        ])
        validation_df.to_csv("data/data_quality_report.csv", index=False, encoding=OUTPUT_CONFIG['CSV_ENCODING'])
        log_analysis_progress("æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ° data/data_quality_report.csv")
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        log_analysis_progress("ğŸš€ å¼€å§‹æ•´è½¦è®¢å•å½’å› åˆ†æ")
        print("=" * 60)
        
        try:
            # 1. æ•°æ®åŠ è½½å’Œæ¸…æ´—
            self.load_and_clean_data()
            
            # 2. åŸºç¡€æ•°æ®æ¢ç´¢
            self.basic_data_exploration()
            
            # 3. é©¬å°”å¯å¤«é“¾å½’å› åˆ†æ
            self.markov_attribution_analysis()
            
            # 4. å¹¶è¡Œåˆ†æï¼ˆå¯é€‰ï¼‰
            if len(self.df) > 10000:  # å¤§æ•°æ®é›†ä½¿ç”¨å¹¶è¡Œåˆ†æ
                log_analysis_progress("æ£€æµ‹åˆ°å¤§æ•°æ®é›†ï¼Œå¯ç”¨å¹¶è¡Œåˆ†æ...")
                self.parallel_analysis()
            
            # 5. æ—¶é—´åºåˆ—åˆ†æ
            self.time_series_analysis()
            
            # 6. é€€è®¢åˆ†æ
            self.cancellation_analysis()
            
            # 7. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self.generate_summary_report()
            
            # 8. ä¿å­˜ç»“æœ
            self.save_results()
            
            log_analysis_progress("âœ… åˆ†æå®Œæˆï¼")
            print("=" * 60)
            
        except Exception as e:
            log_analysis_progress(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", 'error')
            raise

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•´è½¦è®¢å•å½’å› åˆ†æ - ä¼˜åŒ–ç‰ˆæœ¬")
    parser.add_argument("--data", default="data/æ•´è½¦è®¢å•çŠ¶æ€æŒ‡æ ‡è¡¨.xlsx", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--parallel", action="store_true", help="å¯ç”¨å¹¶è¡Œåˆ†æ")
    parser.add_argument("--sparse", action="store_true", help="ä½¿ç”¨ç¨€ç–çŸ©é˜µ")
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    if args.sparse:
        DATA_CONFIG['USE_SPARSE_MATRIX'] = True
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = OptimizedVehicleAttributionAnalysis(args.data)
    analyzer.run_full_analysis()

if __name__ == "__main__":
    main() 